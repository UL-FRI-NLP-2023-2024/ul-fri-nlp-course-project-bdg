{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Fakulteta\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "token = \"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\"\n",
    "\n",
    "huggingface_hub.login(token=token)\n",
    "\n",
    "# huggingface_hub.notebook_login()\n",
    "\n",
    "# api = huggingface_hub.HfApi()\n",
    "# api.set_access_token(token)\n",
    "# folder = huggingface_hub.HfFolder()\n",
    "# folder.save_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n",
    "# tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f041e61a51c4edd83f60e366624111f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "c:\\Users\\Fakulteta\\Desktop\\gitFolder\\fax-stuff\\[MAG] 1. letnik - 2. semester\\NLP\\ul-fri-nlp-course-project-bdg\\.venv\\Lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py:688: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] I'm an artificial intelligence, I don't have the ability to create or cook things myself. However, I can certainly provide you with a classic mayonnaise recipe! Here it is:\\n\\nIngredients:\\n- 1 egg yolk\\n- 1 tablespoon of mustard (Dijon or yellow)\\n- 1 teaspoon of white vinegar or lemon juice\\n- 1 cup of vegetable oil (canola or safflower works\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        # bnb_4bit_compute_dtype=\"torch.float16\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config=quantization_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\",        \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\",   \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\",        \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Yes, I can certainly help you with that! Here's a simple and classic mayonnaise recipe that you can make at home:\\n\\nIngredients:\\n- 1 cup vegetable oil (or a neutral-tasting oil, like canola or sunflower)\\n- 1 egg yolk\\n- 1 tablespoon white wine vinegar\\n- 1 tablespoon Dijon mustard\\n- 1 tablespoon water\\n- 1/\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
