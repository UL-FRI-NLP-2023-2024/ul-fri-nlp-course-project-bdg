[
  {
    "message": "Na Linuxu bi si rad implementiral fake traffic generator. Seveda z omejenim bandwithom, zadeva naj bi npr. \"naključno\" brskala po različnih spletnih straneh, lahko uporabljala tudi druge storitve,... skratka simulirala bi uporabo interneta s strani nekega uporabnika.  Tole je recimo nekaj takega, kar iščem, je pa na voljo samo kot Chrome plugin: https://noiszy.com/  Rabil bi pa nekaj za v bash. ",
    "user": "poweroff"
  },
  {
    "message": "Sem malo pogooglal, termin bi bil ISP data pollution, katerega učinki so sicer omejeni.. edni non-browser link, ki sem ga našel:  https://github.com/essandess/isp-data-p...  mogoče še https://github.com/FascinatedBox/RuinMy... ",
    "user": "Iatromantis"
  },
  {
    "message": "Se zelo strinjam z argumenti glede ISP data pollutiona, vseeno pa nameravam zadevo uporabiti v nekoliko specifičnem primeru. Namreč, zadeva bi tekla na VPNju, ki ga uporablja več oseb (hkrati). Če nekdo spremlja vhodni in izhodni promet, bi lahko delal korelacijo kaj kdo uporablja.  Če je uporabnikov več, je to že težje. Če pa zraven uporabljam še data pollution, pa bi rekel, da je še težje. Hkrati bi bilo seveda fino generirati še nekaj fake prometa znotraj VPNja... ",
    "user": "poweroff"
  },
  {
    "message": "V kakšnem scenariju pa misliš, da bi kdo v real-time spremljal in koreliral dogajanje znotraj VPN povezave?  Endpointa oz. gateway-a imaš običajno pod lastnim nadzorom. Vmes vštuliti se in igrat MITM pri IPSec povezavi pa ni ravno nekaj, kar bi mali Janezek znal.  Niti NSA nebi imela ravno lahko delo, tudi če mogoče imajo na voljo ustrezno tehnologijo, saj bi morali promet nekje preusmeriti na svoje strežnike.  Seveda lahko z 'manjšimi popravki' kompromitirajo VPN gateway, ampak še vedno morajo spraviti promet skozi svoj 'analizator'.   Skratka, če že spravim 'popravek' na gateway, zakaj bi se potem še ubadal z lovljenjem podatkov v VPN-u, če so mi podatki na voljo v nekriptirani obliki na vhodu oz. izhodu iz tunela? Edini razlog za to bi bila stopnja prikritosti - direktno leakanje podatkov se da opaziti, medtem ko se preusmeritev prometa bistveno težje zazna (če ni narejena na kakšen trapast način).  Ali je to zgolj 'teoretično raziskovanje'? ",
    "user": "SeMiNeSanja"
  },
  {
    "message": "Ne. Mogoče nisem dobro opredelil problema.  Imam dva hkratna uporabnika, eden bere BBC news, drugi pa na Youtube gleda filme.  Na izhodu vidiš dva requesta. Za BBC in za Youtube. Hkrati vidiš dva userja, kjer eden prenaša malo podatkov, drugi pa veliko. Ne veš pa kaj prenašata, saj je kriptirano!  Ali pa mogoče veš?  Ja, tisti user, ki prenaša malo podatkov, gleda BBC, tisti user, ki jih prenaša veliko, pa gleda Youtube.  Če na izhodu generiraš fake promet, se to da sicer malo zakriti. Ampak po moje ne dovolj. Že s timingi si lahko napadalec pomaga. Recimo nov user pride in začne nekaj delati in na izhodu se malo spremeni vzorec. Če bi imel več notranjega prometa, bi težje ugotovil, da je za spremembo vzorca odgovoren ta user. Lahko bi bil katerikoli. ",
    "user": "poweroff"
  },
  {
    "message": "Pa si ne moreš pomaga z reverse proxyjem na obeh koncih ?  Že sam varnish bi lahko ustvarjal variacije glede na to ali bo nekaj keširal ali pa ne.  KOneckoncev mu zahtevka tudi ni treba vedno spustiti vedno skozi ravno takoj itd. ",
    "user": "Brane22"
  },
  {
    "message": "Jst sm tak program (za druge namene) naredi, sicer v Pythonu.  Logika gre takole. Prvo si izbereš en keyword. Jst sem štartal recimo z keywordom \"ibiza\", uporabljal sem pa www.google.es. Potem narediš search na googlu in zlistaš prvih nekaj strani rezultatov.  Za vsakega od rezultatov, narediš GET request na to strani in potem zlistaš meta keyworde iz strani. Ti so v: meta name=\"keywords\" content=\"ena, dva, tri\"  Vsakega od teh keywordow dodaš v tisto listo, kjer je \"ibiza\" keyword.  In ko narediš prvi loop (se pravi samo po ibiza keyowrdu), imaš že ene 200 keywordov. In potem reloop, s tem da index prestaviš naprej za tiste, katere je program že prečekiral.  Tak program sem laufal 2 dni in nabral ene 10.000 keywordov. S tem, da program ni prišel do konca. Lahko bi dodal, da bi na redom iskal po različnih www.google.xx straneh, ter neko listo user agentov, ki bi jih tudi na random izbiral itn.  Na tak način je sigurne nekaj traffica, ki ti zamegli vpogled v enega ali dva userja :) ",
    "user": "HotBurek"
  },
  {
    "message": "Sam moraš dobre filtre nastavit, da ne zaide ta tvoj avtomatski mešalec, kam...khm...neprimerno...  ",
    "user": "vostok_1"
  },
  {
    "message": "Na koncu pa vsi jamrajo, da internet nikamor ne gre, bakup da se preko VPN vleče kot megla, ali pa celo pada,.....pa bog ne daj, da bi VOIP fural preko tunela.  Da postane še bolj pestro, ti na koncu še nekdo čez tunel pošlje kakšenga črvička, pa je godlja popolna.  Če je govora o akademskam igračkanju, ni problema. Legitimno razmišljanje in raziskovanje.  Čim smo pa v realnem gospodarstvu, pa ni šanse, da bom preko tunela spuščal stvari, za katere ne morem vedeti kaj so, od koga prihajajo in zakaj pravzaprav gredo čez tunel.  Samo predstavljaj si podjetje, kjer preko VPN povezave omogočajo vzdrževalcem ERP sistema dostop do strežnika, da lahko izvajajo vzdrževalne posege. Pri konfiguraciji VPN boš zelo natančno definiral, kateri računalnik s strani A bo smel komunicirati s strežnikom na strani B - ravno tako pa bo tudi vzdrževalec na svoji strani omejil, da nihče s strani B ne bo mogel preko tunela komunicirati s katerimkoli sistemom na svoji strani. In po možnosti se ne boš omejeval zgolj na source/destination filtrianje, temveč boš želel še kakšno avtentikacijo, če ne celo 2FA, predenj boš spustil promet preko tunela. Vse to pa na loncu tudi logiraš, da imaš neko sled, če enkrat nekaj gre narobe.  Tudi če je oddaljena lokacija nekoliko bolj zaupanja vredna lastna podružnica, ne boš spuščal kar vse preko tunela. Da je to zelo tvegano, je demonstriral Wannacry, ki se je v večjih koncernih širil ravno preko slabo administriranih VPN povezav.  'Anonimizacija' je na tehnološkem nivoju dvorezen meč, ker ti lahko onemogoči pravočasno identifikacijo kakšnega okuženega računalnika ali kakšne druge kompromitacije na omrežju. Vprašanje je bolj pravne narave na nivoju varstva osebnih podatkov, da se jasno definira, kdo kdaj in kako sme dostopati do prometnih podatkov.  Skratka, akademsko razmišljanje razumem. Uporaba v praksi pa no go. Če ima kdo kaj 'mutnega', se naj priključi na nezaščiten javni WiFi. ",
    "user": "SeMiNeSanja"
  },
  {
    "message": "Saj fake traffic je lahko adaptiven. Pač, prioritiziraš promet.  Poleg tega v VPN implementiram NIDS. Sploh pa je glavna finta tega VPNja ta, da se lahko povežeš v free internet od tam, kjer to ni mogoče. Traffic pollution je bolj za zraven. ",
    "user": "poweroff"
  },
  {
    "message": "Kjer to ni mogoče, je običajno že problem vzpostaviti VPN (npr. Kitajska). ",
    "user": "SeMiNeSanja"
  },
  {
    "message": "No, \"moj\" VPN je deloval tudi na Kitajskem. In še v drugih državah, ki cenzurirajo internet. ",
    "user": "poweroff"
  },
  {
    "message": "Že mogoče, da je delal...samo, se baje tisti njihov sistem kar pridno uči. Ni nujno, da bodo stvari, ki danes delujejo, delovale tudi naslednji dan. Vsaj tako sem bral na forumih, da zna stvar biti kar tečna. Potem pa je tu še problem, da ti lahko nekdo potrka na vrata in ponudi 'podaljšanje obiska' pri njih... Ta je bistveno večji, kot da zadeva ne deluje....  Drugje pa zagotovo lažje najdeš luknjo, za katero se ni za bati, da ti po treh dneh ne bo več delovala. ",
    "user": "SeMiNeSanja"
  },
  {
    "message": "To je res. Po drugi strani sem imel kar nekaj časa uporabnike iz neke totalitarne arabske države, ki so se uspešno vozili pod radarjem.  ",
    "user": "poweroff"
  },
  {
    "message": "Ampak tudi v teh totalitarnih državah predvidevam, da se da na nek način doseči, da dobiš nekakšno dovoljenje za uporabo VPN povezave, če se gre za poslovno rabo?  Zelo težko si namreč predstavljam, kako bi sicer potekale resnejše zadeve, kjer potrebuješ določen nivo varnosti. Koneckoncev imajo tudi kitajska podjetja poslovne skrivnosti...  Kakor sem pred časom slišal od nekoga iz WatchGuard-a, ki ima en razvojni laboratorij na Kitajskem, naj bi uporabljali IPSec. Ne vem pa, kako je z 'legalizacijo' teh povezav.  Režimom je koneckoncev v interesu, da blokirajo samo rajo, da nebi dobila ali posredovala preveč informacij, ki režimu niso po godu. Zagotovo pa jim ni v interesu, da bi zaradi tega hromili gospodarstvo. ",
    "user": "SeMiNeSanja"
  },
  {
    "message": "Ja, načeloma lahko dobiš dovoljenje... ampak v mojem primeru je bil uporabnik v vrhu \"prozahodne\" opozicije. Tako da dovoljenja ni mogel dobiti. No, zdaj ni več uporabnik, ker so vodstvo opozicije zaprli, v bistvu je bil on eden redkih, ki mu je uspelo pravočasno zapustiti državo. In mogoče ga niso dobili tudi zato, ker je znal bolj varno uporabljati internet in telefon. ",
    "user": "poweroff"
  },
  {
    "message": "Ali pa je imel zgolj obilico sreče in prijatelje ob pravem času na pravem mestu.....  Koneckoncev režimi kar dobro vedo, kdo jim je trn v očesu. Nekih pretirano trdnih dokazov ne potrebujejo. Zadošča že en špicel (v skrajnem primeru koga tudi tepejo dokler ne pove, kar jih zanima), pa ti še tako dobra tehnika zadaj ne koristi čisto nič.  Dobro kriptiranje in prikrito komuniciranje je bolj pomembno v urejenih juresdikcijah, kjer potrebujejo malo bolj trdne dokaze, da bi te spravili za rešetke.  V totalitarizmih pa po moje služi bolj temu, da sploh lahko izvajaš določene komunikacije, pa da te malo težje locirajo, če že ravno ne skačeš špiclom pred nosom naokoli. Zbiranje dokazov je sekundarnega pomena, če te lahko tepejo, dokler ne priznaš tudi tisto, s čemer nisi imel nobene povezave. ",
    "user": "SeMiNeSanja"
  },
  {
    "message": "Ja, ampak v njegovem primeru je šlo predvsem za to ali bo lahko bral BBC News ali ne. In ga je lahko. ",
    "user": "poweroff"
  },
  {
    "message": " poweroff je 12. okt 2017 ob 13:43 izjavil:Ja, ampak v njegovem primeru je šlo predvsem za to ali bo lahko bral BBC News ali ne. In ga je lahko.  No, saj sem rekel 'omogočanje komunikacij'...  V bistvu bi sem moralo spadati tudi možnost komuniciranja s sorodnimi/naklonjenimi organizacijami v tujini, ki ga tudi praktično ne moreš izvajati brez prikrivanja. Pa še tu se ne gre toliko za to, da se ne izve vsebina, saj takointako vedo, kaj se bo nek aktivist menil npr. z Amnesty International. Režimu je bolj v interesu, da te komunikacije sploh ni. Režimske blokade obvoziti na dolgi rok, pa ni tako enostavno. ",
    "user": "SeMiNeSanja"
  },
  {
    "message": "Evo, ni bash, je pa python.  Spodnja reč, na hitro spisana, generira neke vrste fake traffic, če tako gledaš. V osnovi pa hodi po straneh, išče meta keyworde in jih uporablja na guglu za iskanje novih rezultatov. Rezultati so linki, kamor gre po nove keyworde.  Tako brskanje bi moralo biti dokaj \"varno\", ker obiskuješ strnai, katere najde program preko gugla. Lahko pa tudi zaide kam :)  Path je potrebno popravit, na nekaj, kamor bo program lahko pisal dva fajla. En je index, drugi pa unique keywordi. Poženeš pa preko terminala, lepo v barvah in črno ozadje. :)  Ter, endindex = 20, je koliko strani daleč greš za rezultati. Če več (0-20 je 3 strani), povečuješ po 10.  Start keyword je ibiza, kar lahko spremeniš. Na začetku generira promet ala turizem, hoteli, pizze ipd.  Nekaj podobnega sem poganjal 2 dni, za zbiranje ostalih podatkov (domene, url-je, keywordi, dns to IP ipd.)  In tudi \"parser\" za iskanje linkov bi bilo potrebno izbolšat, ker, kolikor sem testiral, je tudi odvisno od user agenta in ne dela vedno ok.   #!/usr/bin/python3.4\n\n# import\nimport os;\nimport time;\nimport random;\nimport datetime;\nimport urllib.parse;\nimport requests.packages.urllib3;\n\n# define\nuseragents = [\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\",\n              \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\",\n              \"Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16\",\n              \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A\",\n              \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; AS; rv:11.0) like Gecko\"];\nsearchengines = [\"https://www.google.pt\",\n                 \"https://www.google.es\",\n                 \"https://www.google.fr\",\n                 \"https://www.google.it\",\n                 \"https://www.google.si\",\n                 \"https://www.google.at\",\n                 \"https://www.google.de\",\n                 \"https://www.google.lu\",\n                 \"https://www.google.be\",\n                 \"https://www.google.nl\",\n                 \"https://www.google.dk\",\n                 \"https://www.google.se\",\n                 \"https://www.google.no\",\n                 \"https://www.google.fi\",\n                 \"https://www.google.ee\",\n                 \"https://www.google.lt\",\n                 \"https://www.google.lv\",\n                 \"https://www.google.pl\",\n                 \"https://www.google.cz\",\n                 \"https://www.google.sk\"];\naz09chars = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\",\n            \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\",\n            \"u\", \"v\", \"w\", \"x\", \"y\", \"z\",\n            \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n             \" \"];\n\n# paths\npath = \"/home/user/Documents/crawlermini/\";\n\n# start index\nindex = 0;\n\n# colors\nclr_darkgray = \"\\033[90m\";\nclr_red = \"\\033[91m\";\nclr_green = \"\\033[92m\";\nclr_yellow = \"\\033[93m\";\nclr_blue = \"\\033[94m\";\nclr_magenta = \"\\033[95m\";\nclr_azure = \"\\033[96m\";\nclr_lightgray = \"\\033[97m\";\nclr_eoc = \"\\033[0m\";\n\n# clear display\nclear = lambda: os.system(\"clear\");\nclear();\n\n\n# create index file\nif not os.path.exists(path + \"/lastindex\"):\n    file = open(path + \"/lastindex\", \"w\");\n    file.write(\"index=0\" + \"\\n\");\n    file.close();\n\n# create meta file\nif not os.path.exists(path + \"/metakey\"):\n    file = open(path + \"/metakey\", \"w\");\n    file.write(\"ibiza\" + \"\\n\");\n    file.close();\n\nwhile(True):\n    # get random useragent\n    useragent = useragents[random.randint(0, len(useragents) - 1)];\n\n    # get random search engine\n    searchengine = searchengines[random.randint(0, len(searchengines) - 1)];\n\n    # set time and date\n    dtstart = datetime.datetime.now();\n\n    minute = dtstart.minute;\n    if minute < 10:\n        minute = \"0\" + str(minute);\n    hour = dtstart.hour;\n    if hour < 10:\n        hour = \"0\" + str(hour);\n    _time = str(hour) + str(minute);\n\n    day = dtstart.day;\n    if len(str(day)) == 1:\n        day = \"0\" + str(day);\n    month = str(dtstart.month);\n    if len(str(month)) == 1:\n        month = \"0\" + str(month);\n    _date = str(dtstart.year) + str(month) + str(day);\n\n    # read last index and read word out of meta key\n    metakey = \"\";\n    if os.path.exists(path + \"/lastindex\"):\n        file = open(path + \"/lastindex\", \"r\");\n        for line in file.read().split(\"\\n\"):\n            line = str(line);\n            line = line.strip();\n            if line.startswith(\"index=\"):\n                index = line.replace(\"index=\", \"\");\n        file.close();\n    print(\"\\n\" + clr_green + \"index=\" + clr_eoc + clr_lightgray + str(index) + clr_eoc);\n    foundnewkey = False;\n    if os.path.exists(path + \"/metakey\"):\n        file = open(path + \"/metakey\", \"r\");\n        counter = 0;\n        for line in file.read().split(\"\\n\"):\n            if int(counter) == int(index):\n                line = line.strip();\n                metakey = line;\n                if len(metakey) > 0:\n                    foundnewkey = True;\n                    break;\n            else:\n                counter = int(counter) + 1;\n        file.close();\n    if not foundnewkey:\n        print(clr_azure + \"datetime=\" + clr_eoc + clr_lightgray + str(_date) + \" \" + str(_time) + clr_eoc);\n        print(clr_red + \"Time to die.\" + clr_eoc);\n        break;\n    print(clr_green + \"metakey=\" + clr_eoc + clr_lightgray + str(metakey) + clr_eoc);\n\n    # construct request url\n    url = str(searchengine) + str(\"/search?q=\") + urllib.parse.quote_plus(str(metakey));\n\n    # headers\n    headers = {\"User-Agent\": str(useragent)};\n\n    # avoid InsecureRequestWarning\n    requests.packages.urllib3.disable_warnings();\n\n    # page index\n    startindex = 0;\n    endindex = 20;\n\n    # print time and date\n    print(clr_azure + \"datetime=\" + clr_eoc + clr_lightgray + str(_date) + \" \" + str(_time) + clr_eoc);\n\n    while(True):\n\n        # page index url\n        pageindex = \"&start=\" + str(startindex) + \"&*\";\n\n        # print url, user agent\n        print(clr_yellow + \"  url=\" + clr_eoc + clr_lightgray + str(url) + clr_eoc + clr_green + str(pageindex) + clr_eoc);\n        print(clr_yellow + \"  useragent=\" + clr_eoc + clr_lightgray + str(useragent) + clr_eoc);\n\n        # make request to search engine\n        try:\n            request = requests.get(str(url) + str(pageindex), headers=headers, allow_redirects=True, verify=False, timeout=4);\n\n            if request.text:\n                html = request.text;\n                if html.find(\"<body>\") > 0:\n                    html = html[html.find(\"<body>\"):];\n                while(html.find(\"h3 class=\\\"r\\\"\") > 0):\n                    html = html[html.find(\"h3 class=\\\"r\\\"\") + len(\"h3 class=\\\"r\\\"\"):];\n                    href = html[html.find(\"href=\\\"\") + len(\"href=\\\"\"):];\n                    html = html[html.find(\"/h3\") + len(\"/h3\"):];\n                    if href.find(\"\\\"\") > 0:\n                        href = href[:href.find(\"\\\"\")];\n                        if href.startswith(\"http\"):\n                            delay = random.randint(2, 5);\n                            print(clr_magenta + \"  >> Sleeping \" + clr_eoc + clr_yellow + str(delay) + clr_eoc + clr_magenta + \" seconds...\" + clr_eoc);\n                            time.sleep(delay);\n                            print(clr_azure + \"   url=\" + clr_eoc + clr_lightgray + str(href) + clr_eoc);\n                            # call url and list meta keywords\n                            try:\n                                request = requests.get(str(href), headers=headers, allow_redirects=True,verify=False, timeout=4);\n                                if request.text:\n                                    html2 = request.text;\n                                    if html2.find(\"</head>\") > 0:\n                                        html2 = html2[:html2.find(\"</head>\")];\n                                        if html2.find(\"name=\\\"keywords\\\"\") > 0:\n                                            html2 = html2[html2.find(\"name=\\\"keywords\\\"\") + len(\"name=\\\"keywords\\\"\"):];\n                                            if html2.find(\"/>\"):\n                                                html2 = html2[:html2.find(\"/>\")];\n                                                if html2.find(\"content=\\\"\") > 0:\n                                                    html2 = html2[html2.find(\"content=\\\"\") + len(\"content=\\\"\"):];\n                                                    if html2.find(\"\\\"\") > 0:\n                                                        html2 = html2[:html2.find(\"\\\"\")];\n                                                        keywords = html2.split(\",\");\n                                                        for key in keywords:\n                                                            key = key.strip();\n                                                            if len(key) > 0:\n                                                                key = key.lower();\n                                                                # only add key if a-z, 0-9 and space\n                                                                keyaz09 = True;\n                                                                for i in range(0, len(key)):\n                                                                    char = key[i];\n                                                                    if char not in az09chars:\n                                                                        keyaz09 = False;\n                                                                        break;\n                                                                if keyaz09:\n                                                                    unique = True;\n                                                                    file = open(path + \"/metakey\", \"r\");\n                                                                    for line in file.read().split(\"\\n\"):\n                                                                        line = line.strip();\n                                                                        if str(line) == str(key):\n                                                                            unique = False;\n                                                                            break;\n                                                                    file.close();\n                                                                    if unique:\n                                                                        print(clr_yellow + \"    meta=\" + clr_eoc + clr_lightgray + str(key) + clr_eoc);\n                                                                        time.sleep(0.4);\n                                                                        file = open(path + \"/metakey\", \"a+\");\n                                                                        file.write(key + \"\\n\");\n                                                                        file.close();\n                            except Exception as error:\n                                print(clr_red + \"error=\" + clr_eoc + clr_lightgray + str(error) + clr_eoc);\n\n        except Exception as error:\n            print(clr_red + \"error=\" + clr_eoc + clr_lightgray + str(error) + clr_eoc);\n\n        # sleep between google calls\n        print(clr_magenta + \"  > Sleeping \" + clr_eoc + clr_yellow + \"4\" + clr_eoc + clr_magenta + \" seconds...\" + clr_eoc + \"\\n\");\n        time.sleep(4);\n\n        # move index forward\n        if(int(startindex) < int(endindex)):\n            startindex = int(startindex) + 10;\n        else:\n            startindex = 0;\n            # shift to new metaindex\n            index = int(index) + 1;\n            file = open(path + \"/lastindex\", \"w\");\n            file.write(\"index=\" + str(index) + \"\\n\");\n            file.close();\n            break;\n ",
    "user": "HotBurek"
  }
]